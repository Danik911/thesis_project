#!/usr/bin/env python3
"""
Test Phoenix Enhanced Observability with Workflow Integration

This script tests the complete Phoenix enhanced observability integration
with a real workflow execution, validating all features work end-to-end.
"""

import asyncio
import sys
from pathlib import Path

# Add main to path
sys.path.append(str(Path.cwd() / "main"))

async def test_enhanced_phoenix_workflow():
    """Test the complete enhanced Phoenix observability workflow."""
    
    print("ğŸš€ PHOENIX ENHANCED OBSERVABILITY WORKFLOW TEST")
    print("=" * 70)
    
    # Test 1: Import validation
    print("\n1. Testing imports...")
    try:
        from src.monitoring.phoenix_enhanced import (
            PhoenixGraphQLClient,
            WorkflowEventFlowVisualizer,
            AutomatedTraceAnalyzer,
            setup_enhanced_phoenix_observability
        )
        print("   âœ… Enhanced Phoenix imports successful")
    except ImportError as e:
        print(f"   âŒ Import failed: {e}")
        return False
    
    # Test 2: Phoenix server check
    print("\n2. Checking Phoenix server...")
    try:
        import aiohttp
        async with aiohttp.ClientSession() as session:
            async with session.get("http://localhost:6006", timeout=aiohttp.ClientTimeout(total=5)) as response:
                if response.status == 200:
                    print("   âœ… Phoenix server is running")
                    server_running = True
                else:
                    print(f"   âš ï¸  Phoenix server status: {response.status}")
                    server_running = False
    except Exception as e:
        print(f"   âŒ Phoenix server not accessible: {e}")
        print("   ğŸš€ Start with: uv run python start_phoenix_server.py")
        server_running = False
    
    if not server_running:
        print("\nâŒ Cannot proceed without Phoenix server running")
        return False
    
    # Test 3: Enhanced observability setup
    print("\n3. Testing enhanced observability setup...")
    try:
        enhanced_setup = await setup_enhanced_phoenix_observability()
        
        if enhanced_setup.get("status") == "ready":
            print("   âœ… Enhanced observability setup successful")
            print(f"   ğŸ“‹ Capabilities: {len(enhanced_setup.get('capabilities', []))}")
            
            # Get clients
            graphql_client = enhanced_setup["graphql_client"]
            visualizer = enhanced_setup["visualizer"]
            analyzer = enhanced_setup["analyzer"]
            
        else:
            print(f"   âŒ Setup failed: {enhanced_setup}")
            return False
            
    except Exception as e:
        print(f"   âŒ Enhanced setup failed: {e}")
        return False
    
    # Test 4: Basic GraphQL queries
    print("\n4. Testing GraphQL functionality...")
    try:
        # Test compliance metrics
        metrics = await graphql_client.query_compliance_metrics(timeframe_hours=1)
        print(f"   âœ… Compliance metrics query: {len(metrics)} metrics")
        
        # Test workflow traces  
        traces = await graphql_client.query_workflow_traces(hours=1)
        print(f"   âœ… Workflow traces query: {len(traces)} traces")
        
    except Exception as e:
        print(f"   âŒ GraphQL queries failed: {e}")
        print(f"   ğŸ“‹ Error details: {type(e).__name__}: {e}")
        return False
    
    # Test 5: Compliance analysis
    print("\n5. Testing compliance analysis...")
    try:
        violations = await analyzer.analyze_compliance_violations(hours=1)
        print(f"   âœ… Compliance analysis: {len(violations)} violations found")
        
        if violations:
            for i, violation in enumerate(violations[:3]):  # Show first 3
                print(f"      {i+1}. {violation.violation_type}: {violation.severity}")
        else:
            print("   âœ… No compliance violations detected")
            
    except Exception as e:
        print(f"   âŒ Compliance analysis failed: {e}")
        return False
    
    # Test 6: Dashboard generation
    print("\n6. Testing dashboard generation...")
    try:
        dashboard_path = await visualizer.create_compliance_dashboard()
        print(f"   âœ… Dashboard generated: {dashboard_path}")
        
        # Check if file exists
        if Path(dashboard_path).exists():
            print("   âœ… Dashboard file exists")
        else:
            print("   âš ï¸  Dashboard file not found")
            
    except Exception as e:
        print(f"   âŒ Dashboard generation failed: {e}")
        return False
    
    # Test 7: Comprehensive report
    print("\n7. Testing comprehensive compliance report...")
    try:
        report = await analyzer.generate_compliance_report()
        
        metadata = report.get("report_metadata", {})
        summary = report.get("compliance_summary", {})
        violations_summary = report.get("violations_summary", {})
        
        print(f"   âœ… Report generated successfully")
        print(f"   ğŸ“Š Compliance rate: {summary.get('compliance_rate_percent', 0):.1f}%")
        print(f"   ğŸš¨ Total violations: {violations_summary.get('total_violations', 0)}")
        print(f"   âš–ï¸  Regulatory status: {metadata.get('regulatory_status', 'UNKNOWN')}")
        
    except Exception as e:
        print(f"   âŒ Report generation failed: {e}")
        return False
    
    # Test 8: Integration test with mock workflow data
    print("\n8. Testing workflow integration simulation...")
    try:
        # Simulate workflow completion with enhanced observability
        mock_workflow_results = {
            "session_id": "test_session_001",
            "workflow_type": "UnifiedTestGenerationWorkflow",
            "enhanced_observability": {
                "compliance_dashboard": dashboard_path,
                "compliance_report": report,
                "trace_count": len(traces)
            },
            "compliance_violations": violations,
            "regulatory_compliance": {
                "gamp_5_compliant": len([v for v in violations if getattr(v, 'severity', 'UNKNOWN') == 'CRITICAL']) == 0,
                "audit_trail_complete": True,
                "pharmaceutical_validation_ready": len(violations) == 0
            }
        }
        
        print("   âœ… Workflow integration simulation successful")
        print(f"   ğŸ“‹ GAMP-5 compliant: {mock_workflow_results['regulatory_compliance']['gamp_5_compliant']}")
        print(f"   ğŸ“‹ Validation ready: {mock_workflow_results['regulatory_compliance']['pharmaceutical_validation_ready']}")
        
    except Exception as e:
        print(f"   âŒ Integration simulation failed: {e}")
        return False
    
    # Final status
    print("\n" + "=" * 70)
    print("ğŸ¯ PHOENIX ENHANCED OBSERVABILITY TEST RESULTS")
    print("=" * 70)
    print("âœ… All enhanced Phoenix observability features are working!")
    print("ğŸ“Š Dashboard generation: FUNCTIONAL")
    print("ğŸ” Compliance analysis: FUNCTIONAL") 
    print("ğŸ“‹ Comprehensive reporting: FUNCTIONAL")
    print("ğŸ”— GraphQL integration: FUNCTIONAL")
    print("âš–ï¸  Regulatory compliance tracking: FUNCTIONAL")
    
    print("\nğŸš€ READY FOR PRODUCTION INTEGRATION!")
    print("ğŸ“‹ Next steps:")
    print("   1. Enhanced features are fully functional")
    print("   2. Integration patch can be applied to unified_workflow.py")
    print("   3. End-to-end testing with real workflow data")
    
    print("=" * 70)
    return True

if __name__ == "__main__":
    success = asyncio.run(test_enhanced_phoenix_workflow())
    
    if success:
        print("\nâœ… ALL TESTS PASSED - Phoenix Enhanced Observability is ready!")
    else:
        print("\nâŒ TESTS FAILED - Review issues above")
    
    sys.exit(0 if success else 1)