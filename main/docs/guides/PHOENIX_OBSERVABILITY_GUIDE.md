# Phoenix Observability Guide for AI Agents
## GAMP-5 Pharmaceutical Test Generation System

This guide provides comprehensive instructions for AI agents working with the Phoenix observability system in the pharmaceutical test generation workflow.

---

## 🎯 Overview

The GAMP-5 Pharmaceutical Test Generation System includes production-grade Phoenix observability for monitoring multi-agent workflows, LLM interactions, and compliance tracking. This system provides real-time visibility into:

- Multi-agent workflow execution (GAMP categorization → Planning → Execution)
- LLM API calls with token usage and cost tracking
- Performance metrics and bottleneck identification
- Error tracking with detailed context
- GAMP-5 compliance audit trails

---

## 🚀 Quick Start for AI Agents

### Prerequisites Check
```bash
# Verify environment
python -c "import openai; print('OpenAI available')"
python -c "from dotenv import load_dotenv; load_dotenv(); import os; print('API Key:', 'SET' if os.getenv('OPENAI_API_KEY') else 'MISSING')"
```

### Docker Phoenix Setup (Recommended)
The system supports both local Phoenix and Docker Phoenix instances:

```bash
# Start Phoenix in Docker (recommended)
docker run -d -p 6006:6006 arizephoenix/phoenix:latest

# Verify Docker Phoenix is running
docker ps | grep phoenix
curl -f http://localhost:6006 && echo "Phoenix UI accessible"
```

### Basic Initialization
The observability system is automatically initialized when running the main workflow:

```bash
# Run with observability enabled (default) - automatically detects Docker Phoenix
uv run python main/main.py test_phoenix.txt --categorization-only

# For full workflow test
uv run python main/main.py test_phoenix.txt --verbose

# Check Phoenix UI availability
curl -f http://localhost:6006 || echo "Phoenix UI not accessible"
```

### Expected Output
```
🏥 GAMP-5 Pharmaceutical Test Generation System
🚀 Running Unified Test Generation Workflow
============================================================
📊 Setting up event logging system...
✅ Connected to existing Phoenix instance at: http://localhost:6006
🌍 To view the Phoenix app in your browser, visit http://localhost:6006/
📖 For more information on how to use Phoenix, check out https://arize.com/docs/phoenix
🔭 OpenTelemetry Tracing Details 🔭
|  Phoenix Project: test_generation_thesis
|  Span Processor: SimpleSpanProcessor
|  Collector Endpoint: http://localhost:6006/v1/traces
|  Transport: HTTP + protobuf
|  Transport Headers: {'api-key': '****', 'authorization': '****'}
```

---

## 🔧 Architecture Components

### Core Files and Responsibilities

#### 1. Phoenix Configuration (`src/monitoring/phoenix_config.py`)
```python
# Main Phoenix configuration and manager
class PhoenixConfig:
    - phoenix_host: str = "localhost"
    - phoenix_port: int = 6006
    - enable_tracing: bool = True
    - service_name: str = "test_generator"
    - project_name: str = "test_generation_thesis"
    - phoenix_api_key: str = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."

class PhoenixManager:
    - setup(): Initialize Phoenix with OpenTelemetry
    - _launch_local_phoenix(): Auto-detect Docker/local Phoenix instances
    - _setup_tracer(): Configure OTLP exporter with phoenix.otel.register
    - _instrument_llamaindex(): Enable LlamaIndex tracing with fallbacks
    - shutdown(): Graceful shutdown with force_flush for trace persistence
```

#### 2. Event Logging Integration (`src/shared/event_logging.py`)
```python
# Phoenix integration in event logging
def setup_event_logging(config: SystemConfig) -> EventStreamHandler:
    if config.phoenix.enable_phoenix:
        # Convert config and initialize Phoenix
        monitoring_config = MonitoringPhoenixConfig(...)
        phoenix_manager = setup_phoenix(monitoring_config)
```

#### 3. Categorization Agent (`src/agents/categorization/agent.py`)
```python
# Automatic instrumentation via LlamaIndex
# All LLM calls are traced automatically when Phoenix is enabled
```

---

## 📋 Environment Configuration

### Required Environment Variables
Add to your `.env` file:
```bash
# OpenAI API (Required)
OPENAI_API_KEY=your_openai_api_key_here

# Phoenix Configuration (Optional - defaults provided)
PHOENIX_HOST=localhost
PHOENIX_PORT=6006
PHOENIX_ENABLE_TRACING=true
PHOENIX_ENABLE_LOCAL_UI=true

# Project Identification
PHOENIX_PROJECT_NAME=test_generation_thesis
PHOENIX_EXPERIMENT_NAME=multi_agent_workflow

# OpenTelemetry Configuration
OTEL_SERVICE_NAME=test_generator
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:6006/v1/traces
```

### Optional Advanced Configuration
```bash
# Performance Tuning
PHOENIX_BATCH_SPAN_PROCESSOR_MAX_QUEUE_SIZE=2048
PHOENIX_BATCH_SPAN_PROCESSOR_MAX_EXPORT_BATCH_SIZE=512
PHOENIX_BATCH_SPAN_PROCESSOR_SCHEDULE_DELAY_MILLIS=5000

# Compliance Settings
PHOENIX_ENABLE_COMPLIANCE_ATTRIBUTES=true
PHOENIX_ENABLE_PII_FILTERING=true

# Environment
DEPLOYMENT_ENVIRONMENT=development
SERVICE_VERSION=1.0.0
```

---

## 🔍 Key Observability Features

### 1. Multi-Agent Workflow Tracing
- **GAMP Categorization**: Tracks categorization logic and confidence scoring
- **Test Planning**: Monitors planning decisions and strategy generation
- **Agent Coordination**: Traces agent handoffs and parallel execution
- **Error Recovery**: Captures fallback mechanisms and error handling

### 2. LLM Call Monitoring
- **Token Usage**: Real-time tracking of prompt, completion, and total tokens
- **Cost Estimation**: Automatic cost calculation based on token usage
- **Prompt Analysis**: Full prompt and response logging for optimization
- **Model Performance**: Response times and throughput metrics

### 3. GAMP-5 Compliance Tracking
- **Audit Trails**: Complete execution history for regulatory compliance
- **Decision Rationale**: Detailed reasoning for all categorization decisions
- **Error Documentation**: Comprehensive error logging with context
- **Confidence Scoring**: Tracking of all confidence thresholds and decisions

### 4. Enhanced Phoenix Integration (NEW)
- **Docker Support**: Automatic detection and connection to Docker Phoenix instances
- **Fallback Mechanisms**: Multi-level fallbacks (OpenInference → global handler → graceful degradation)
- **Force Flush**: Ensures trace persistence with configurable timeout
- **Production Ready**: BatchSpanProcessor optimization and error recovery
- **API Key Authentication**: Secure trace transmission with API key headers

---

## 🚨 Common Issues and Solutions

### Issue 1: Phoenix UI Not Launching
**Symptoms:**
```
ERROR - Failed to initialize Phoenix: 'PhoenixConfig' object has no attribute 'to_resource_attributes'
```

**Root Cause:** Configuration mismatch between simple and monitoring configs

**Solution:**
```python
# Already fixed in src/shared/event_logging.py
# Converts simple config to monitoring config automatically
monitoring_config = MonitoringPhoenixConfig(
    phoenix_host=config.phoenix.phoenix_host,
    phoenix_port=config.phoenix.phoenix_port,
    # ... other mappings
)
```

**Verification:**
```bash
# Should show no errors
uv run python main/main.py simple_test_data.md | grep -i "phoenix"
```

### Issue 2: Missing Dependencies
**Symptoms:**
```
ImportError: No module named 'openinference'
WARNING - OpenInference LlamaIndex instrumentation not available
```

**Solution:**
```bash
# Install required packages
pip install arize-phoenix>=4.0.0
pip install openinference-instrumentation-llama-index>=2.0.0
pip install tiktoken

# Verify installation
python -c "import phoenix as px; print('Phoenix available')"
python -c "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor; print('OpenInference available')"
```

### Issue 3: OpenAI API Key Issues
**Symptoms:**
```
AuthenticationError: Error code: 401 - {'error': {'message': "You didn't provide an API key"
```

**Root Cause:** Environment variables not loaded properly

**Solution (Already Fixed):**
```python
# In main/main.py (line 15-16)
from dotenv import load_dotenv
load_dotenv()
```

**Verification:**
```bash
# Check if API key is loaded
python -c "from dotenv import load_dotenv; load_dotenv(); import os; print('API Key loaded:', bool(os.getenv('OPENAI_API_KEY')))"
```

### Issue 4: Port Already in Use
**Symptoms:**
```
ERROR - Failed to launch Phoenix UI: [Errno 48] Address already in use
```

**Solutions:**
```bash
# Option 1: Kill existing Phoenix process
lsof -ti:6006 | xargs kill -9

# Option 2: Use different port
export PHOENIX_PORT=6007
export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:6007/v1/traces

# Option 3: Connect to existing Phoenix instance
# (Current implementation automatically does this)
```

### Issue 5: Agent Response Parsing Issues
**Symptoms:**
```
ValueError: Could not extract category from agent response
WARNING - Categorization fallback triggered: llm_error
```

**Root Cause:** Agent response format not matching regex patterns

**Solution (Already Fixed):**
```python
# Enhanced regex patterns in src/agents/categorization/agent.py
category_match = re.search(r"[Cc]ategory[\s:]*(\d)", response_text)
if not category_match:
    # Try markdown format
    category_match = re.search(r"\*\*Determined Category\*\*[\s:]*(\d)", response_text)
```

### Issue 6: Max Iterations Reached
**Symptoms:**
```
Max iterations of 20 reached - categorization completely failed
```

**Root Cause:** FunctionAgent getting stuck in tool calling loops

**Solution (Already Fixed):**
```python
# Enhanced system prompt with explicit instructions
system_prompt = """
IMPORTANT INSTRUCTIONS:
1. First, call the gamp_analysis_tool with the URS content
2. Then, call the confidence_tool with the complete analysis results
3. After calling BOTH tools EXACTLY ONCE, provide your final answer

DO NOT call any tool more than once.
"""

# Increased max_iterations from 10 to 15
agent = FunctionAgent(
    tools=[...],
    max_iterations=15  # Sufficient for both tool calls
)
```

---

## 📊 Monitoring and Debugging

### Health Check Commands
```bash
# System status
python -c "from src.monitoring.phoenix_config import setup_phoenix; print('Phoenix setup successful')"

# Test categorization agent
uv run python main/tests/debug_categorization_loop.py

# Check Phoenix UI
curl -f http://localhost:6006 && echo "Phoenix UI accessible"
```

### Debug Logging
```bash
# Enable debug logging
export PHOENIX_VERBOSE_LOGGING=true

# Run with detailed output
uv run python main/main.py simple_test_data.md --verbose
```

### Performance Monitoring
```bash
# Monitor token usage
grep -i "token" logs/events/*.log

# Check execution times
grep -i "duration" logs/events/*.log

# Monitor API calls
grep -i "API" logs/events/*.log
```

---

## 📚 Documentation Links

### Official Phoenix Documentation
- **Main Documentation**: https://arize.com/docs/phoenix
- **LlamaIndex Integration**: https://arize.com/docs/phoenix/integrations/frameworks/llamaindex/llamaindex-tracing
- **OpenTelemetry Setup**: https://arize.com/docs/phoenix/tracing/how-to-tracing/setup-tracing
- **Troubleshooting Guide**: https://arize.com/docs/phoenix/troubleshooting

### LlamaIndex Resources
- **Observability Guide**: https://docs.llamaindex.ai/en/stable/examples/cookbooks/oreilly_course_cookbooks/Module-5/Observability/
- **Instrumentation Examples**: https://docs.llamaindex.ai/en/stable/module_guides/observability/
- **Workflow Monitoring**: https://docs.llamaindex.ai/en/stable/module_guides/workflow/

### OpenInference Specifications
- **GitHub Repository**: https://github.com/Arize-ai/openinference
- **Instrumentation Packages**: https://github.com/Arize-ai/openinference/tree/main/python/instrumentation
- **LLM Tracing Standards**: https://github.com/Arize-ai/openinference/blob/main/spec/semantic_conventions.md

### Project-Specific Documentation
- **Old Implementation Guide**: `/home/anteb/thesis_project/test_generation/examples/old_docs/PHOENIX_OBSERVABILITY.md`
- **Monitoring Plan**: `/home/anteb/thesis_project/test_generation/examples/old_docs/issues/OBSERVABILITY_MONITORING_PLAN.md`
- **LlamaIndex Tracing Examples**: `/home/anteb/thesis_project/test_generation/examples/notebooks/llamaindex_taracing.md`

---

## 🔧 Code Examples

### Manual Phoenix Setup (Advanced)
```python
from src.monitoring.phoenix_config import PhoenixConfig, PhoenixManager

# Create custom configuration
config = PhoenixConfig(
    phoenix_host="localhost",
    phoenix_port=6006,
    service_name="custom_agent",
    project_name="custom_project",
    enable_tracing=True
)

# Initialize Phoenix
manager = PhoenixManager(config)
manager.setup()

# Get tracer for manual instrumentation
tracer = manager.get_tracer(__name__)

# Create manual spans
with tracer.start_as_current_span("custom_operation") as span:
    span.set_attribute("operation.type", "gamp_categorization")
    span.set_attribute("confidence.score", 0.85)
    # Your code here
```

### Custom Event Logging
```python
from src.shared.event_logging import setup_event_logging
from src.shared.config import SystemConfig

# Initialize with Phoenix enabled
config = SystemConfig()
config.phoenix.enable_phoenix = True
handler = setup_event_logging(config)

# Log custom events
handler.log_event({
    "event_type": "categorization_complete",
    "category": 4,
    "confidence": 0.92,
    "timestamp": datetime.now(UTC).isoformat()
})
```

### Workflow Integration
```python
from llama_index.core.workflow import Workflow, Context, StartEvent, StopEvent

# Phoenix automatically instruments Workflow classes
class CustomWorkflow(Workflow):
    async def step_with_tracing(self, ctx: Context, ev: StartEvent):
        # All LLM calls are automatically traced
        response = await self.llm.agenerate(["Your prompt here"])
        
        # Manual span attributes (optional)
        if hasattr(ctx, 'span') and ctx.span:
            ctx.span.set_attribute("custom.metric", "value")
        
        return StopEvent(result=response)
```

---

## 🎯 Best Practices for AI Agents

### 1. Always Check Phoenix Status
```python
def check_observability_status():
    """Check if Phoenix observability is working."""
    try:
        from src.monitoring.phoenix_config import setup_phoenix
        manager = setup_phoenix()
        return manager._initialized
    except Exception as e:
        print(f"Observability check failed: {e}")
        return False
```

### 2. Handle Missing Dependencies Gracefully
```python
def safe_phoenix_operation():
    """Perform operations with Phoenix fallback."""
    try:
        # Phoenix-enabled operation
        from openinference.instrumentation.llama_index import LlamaIndexInstrumentor
        # Your Phoenix-specific code
        return perform_traced_operation()
    except ImportError:
        # Fallback without Phoenix
        return perform_standard_operation()
```

### 3. Monitor Token Usage
```python
def check_token_usage():
    """Monitor token usage for cost control."""
    try:
        from llama_index.core.callbacks import TokenCountingHandler
        token_counter = TokenCountingHandler()
        
        # Add to callback manager
        from llama_index.core import Settings
        Settings.callback_manager.add_handler(token_counter)
        
        return token_counter
    except Exception as e:
        print(f"Token monitoring setup failed: {e}")
        return None
```

### 4. Validate Configuration
```python
def validate_phoenix_config():
    """Validate Phoenix configuration before use."""
    required_vars = [
        "OPENAI_API_KEY",
        "PHOENIX_HOST",
        "PHOENIX_PORT"
    ]
    
    missing = []
    for var in required_vars:
        if not os.getenv(var):
            missing.append(var)
    
    if missing:
        print(f"Missing environment variables: {missing}")
        return False
    
    return True
```

---

## 🔍 Testing and Validation

### Unit Tests
```python
# Test Phoenix initialization
def test_phoenix_setup():
    from src.monitoring.phoenix_config import PhoenixConfig, setup_phoenix
    
    config = PhoenixConfig(enable_tracing=True)
    manager = setup_phoenix(config)
    
    assert manager is not None
    # Note: May not be initialized if dependencies missing
    print(f"Phoenix initialized: {manager._initialized}")

# Test categorization with observability
def test_categorization_with_tracing():
    from src.agents.categorization.agent import create_gamp_categorization_agent
    
    agent = create_gamp_categorization_agent(verbose=True)
    # Test should work with or without Phoenix
    assert agent is not None
```

### Integration Tests
```bash
# Test full workflow with observability
uv run python main/main.py simple_test_data.md

# Verify Phoenix UI is accessible
curl -f http://localhost:6006/health

# Check for traces in Phoenix
curl -f http://localhost:6006/v1/traces
```

---

## 🚀 Performance Optimization

### Configuration Tuning
```python
# For high-throughput scenarios
config = PhoenixConfig(
    batch_span_processor_max_queue_size=4096,
    batch_span_processor_max_export_batch_size=1024,
    batch_span_processor_schedule_delay_millis=1000  # Faster export
)

# For development/debugging
config = PhoenixConfig(
    batch_span_processor_schedule_delay_millis=100,  # Near real-time
    enable_local_ui=True,
    deployment_environment="development"
)
```

### Memory Management
```python
# Cleanup after workflow completion
def cleanup_phoenix():
    """Clean up Phoenix resources."""
    from src.monitoring.phoenix_config import _phoenix_manager
    
    if _phoenix_manager:
        _phoenix_manager.shutdown()
        _phoenix_manager = None
```

---

## 🔒 Security and Compliance

### GAMP-5 Compliance Features
- **Audit Trails**: All decisions logged with timestamps and reasoning
- **Data Integrity**: Immutable trace records in Phoenix
- **Error Tracking**: Complete error context for compliance review
- **Confidence Tracking**: All confidence scores recorded for validation

### PII and Security
```python
# PII filtering is enabled by default
config = PhoenixConfig(
    enable_pii_filtering=True,        # Filter sensitive data
    enable_compliance_attributes=True  # Add compliance metadata
)
```

### Access Control
```bash
# Phoenix UI access control (if needed in production)
export PHOENIX_API_KEY=your_secure_api_key
```

---

## 🎉 Success Indicators

### What Success Looks Like
1. **Phoenix UI Accessible**: http://localhost:6006 loads without errors
2. **Workflow Completion**: No "Failed to initialize Phoenix" errors
3. **Trace Data**: Spans visible in Phoenix UI during/after workflow execution
4. **Token Tracking**: Real-time token usage displayed
5. **Performance Metrics**: Execution times and throughput data available

### Validation Checklist
- [ ] Environment variables loaded correctly
- [ ] Phoenix UI launches on port 6006
- [ ] LlamaIndex instrumentation active
- [ ] Agent responses parsed correctly
- [ ] Token usage tracked accurately
- [ ] Error handling working with fallbacks
- [ ] Compliance attributes recorded
- [ ] No infinite iteration loops

---

## 🔄 Recent Updates (2025-07-29)

### ✅ Enhanced Phoenix Implementation
- **Docker Integration**: Added automatic Docker Phoenix detection and connection
- **Improved Reliability**: Multi-level fallback mechanisms for production stability
- **Trace Persistence**: Force flush functionality ensures traces are saved
- **Better Error Handling**: Graceful degradation when Phoenix is unavailable
- **Production Optimized**: BatchSpanProcessor settings for high-throughput scenarios

### 🧪 Verified Functionality
- ✅ Docker Phoenix connection working
- ✅ OpenTelemetry traces successfully sent to Phoenix
- ✅ LlamaIndex workflow instrumentation active
- ✅ GAMP-5 pharmaceutical workflow tracing end-to-end
- ✅ Proper shutdown with trace persistence
- ✅ UI accessibility maintained post-workflow

---

*Last Updated: 2025-07-29*  
*System Status: Production Ready*  
*Phoenix Version: 4.0.0+*  
*LlamaIndex Version: 0.12.0+*  
*Docker Support: Active*