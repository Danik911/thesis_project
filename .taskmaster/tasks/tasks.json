{
  "tasks": [
    {
      "id": 1,
      "title": "Fix Categorization Agent Fallback Violations",
      "description": "Remove ALL fallback logic from categorization agent that violates NO FALLBACKS policy",
      "status": "pending",
      "dependencies": [],
      "priority": "high",
      "details": "The categorization agent currently has prohibited fallback logic that hides real failures by automatically assigning Category 5 and artificial confidence scores. Must remove all fallback mechanisms (lines 309-316, 378-379, 769-770, system prompt line 478) and replace with explicit failure modes that expose full diagnostic information. This is CRITICAL as it blocks all downstream work.",
      "testStrategy": "For each fallback mechanism removed, verify that: (1) no automatic Category 5 assignment occurs on exceptions, (2) no artificial confidence values (0.3 or 0.7) are injected, (3) the system prompt contains no fallback instructions, and (4) explicit error or failure information is surfaced in logs or outputs. Add unit and integration tests to confirm that failures are not masked and that diagnostic information is available.",
      "subtasks": [
        {
          "id": 5,
          "title": "Remove automatic Category 5 fallback (lines 309-316)",
          "description": "Remove the error handler that returns Category 5 on any exception. Ensure that exceptions are surfaced explicitly and not masked by fallback categorization.",
          "dependencies": [],
          "details": "Locate and delete the code block (lines 309-316) that catches exceptions and returns Category 5. Replace with logic that surfaces the exception and provides diagnostic information without assigning a fallback category.",
          "status": "pending"
        },
        {
          "id": 6,
          "title": "Remove artificial confidence injection (lines 378-379)",
          "description": "Remove the code that returns a default confidence value of 0.3. Ensure that confidence is only set based on actual model output or explicit error reporting.",
          "dependencies": [],
          "details": "Delete lines 378-379 where a default confidence of 0.3 is returned. Update logic to avoid assigning confidence unless it is derived from a valid categorization process.",
          "status": "pending"
        },
        {
          "id": 7,
          "title": "Remove default confidence on parse failure (lines 769-770)",
          "description": "Remove the code that assigns a default confidence of 0.7 on parse failure. Ensure parse failures are reported explicitly.",
          "dependencies": [],
          "details": "Delete lines 769-770 where a default confidence of 0.7 is assigned on parse failure. Update error handling to surface parse errors without assigning fallback confidence.",
          "status": "pending"
        },
        {
          "id": 8,
          "title": "Update system prompt to remove fallback instructions (line 478)",
          "description": "Remove the instruction in the system prompt that directs the agent to assign Category 5 on low confidence.",
          "dependencies": [],
          "details": "Edit the system prompt at line 478 to eliminate any mention of assigning Category 5 as a fallback or on low confidence. Ensure the prompt reflects the NO FALLBACKS policy.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 2,
      "title": "Implement Pydantic Structured Output for Categorization",
      "description": "Replace fragile regex parsing with Pydantic models for structured LLM output",
      "details": "Current categorization agent uses complex regex patterns (lines 740-805) to extract category and confidence from natural language. This is fragile and error-prone. Replace with Pydantic models using LLMTextCompletionProgram for guaranteed structured output. Must NOT use response_format json_object with FunctionAgent.",
      "testStrategy": "",
      "status": "pending",
      "dependencies": [
        1
      ],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 3,
      "title": "Integrate Context Provider as Categorization Tool",
      "description": "Add context_provider agent as a tool to boost categorization confidence",
      "details": "Integrate the existing context_provider agent as a FunctionTool within the categorization agent. This will provide access to GAMP-5 regulatory knowledge base, precedent matching, and guidance validation. Expected confidence boost of +0.15 to +0.20. Implementation should query context provider after initial analysis to validate and enhance categorization decisions.",
      "testStrategy": "",
      "status": "pending",
      "dependencies": [
        2
      ],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 4,
      "title": "Test Categorization Fixes on URS Test Cases",
      "description": "Validate categorization agent fixes on URS-001 through URS-005 test cases",
      "details": "Test the fixed categorization agent on the test cases in testing_data.md: URS-001 (clear Category 3), URS-002 (clear Category 4), URS-003 (clear Category 5), URS-004 (ambiguous 3/4), URS-005 (ambiguous 4/5). Verify correct categorization with improved confidence scores and no fallback behaviors.",
      "testStrategy": "",
      "status": "pending",
      "dependencies": [
        3
      ],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 5,
      "title": "Implement OQ Test-Script Generation Agent",
      "description": "Create test-script generation agent focused on OQ (Operational Qualification) tests only",
      "status": "pending",
      "dependencies": [
        4
      ],
      "priority": "high",
      "details": "Implement the critical test-script generation agent using LlamaIndex Workflow with event-driven architecture. Focus ONLY on OQ test generation (no IQ/PQ/RTM). Use Pydantic models for structured test output. Implement GAMP category-specific OQ templates (5-10 tests for Cat 3, 15-20 for Cat 4, 25-30 for Cat 5). Must aggregate context from all upstream agents. NO response_format json_object - use LLMTextCompletionProgram.",
      "testStrategy": "1. Unit test Pydantic models for schema correctness and validation logic.\n2. Integration test the OQTestGenerationWorkflow to ensure correct event-driven orchestration and output structure.\n3. Validate generated OQ test scripts against GAMP requirements for each category (3, 4, 5).\n4. Test context aggregation by simulating upstream agent outputs (URS, categorization, planning, parallel agents).\n5. Review generated test scripts for pharmaceutical compliance and completeness.",
      "subtasks": [
        {
          "id": 5,
          "title": "Create Pydantic Models for OQ Test Structure",
          "description": "Define Pydantic models for OQTestStep, OQTestScript, and any related entities to ensure structured, validated test output.",
          "dependencies": [],
          "details": "Design and implement Pydantic models representing individual OQ test steps and complete OQ test scripts. Ensure models capture all required fields for pharmaceutical OQ compliance and support validation logic.",
          "status": "pending"
        },
        {
          "id": 6,
          "title": "Implement OQ Test Generation Workflow",
          "description": "Develop the OQTestGenerationWorkflow using LlamaIndex event-driven patterns to orchestrate OQ test creation.",
          "dependencies": [
            5
          ],
          "details": "Use LlamaIndex Workflow to define event-driven steps for OQ test generation. Ensure each step processes and emits events as required, and the workflow produces structured OQ test scripts using the defined Pydantic models.",
          "status": "pending"
        },
        {
          "id": 7,
          "title": "Develop GAMP-Specific OQ Templates",
          "description": "Create OQ test templates for GAMP categories: 5-10 tests for Cat 3, 15-20 for Cat 4, 25-30 for Cat 5.",
          "dependencies": [
            6
          ],
          "details": "Design and implement OQ test templates tailored to GAMP category requirements. Ensure templates are parameterized and can be populated dynamically within the workflow.",
          "status": "pending"
        },
        {
          "id": 8,
          "title": "Integrate Context from Upstream Agents",
          "description": "Aggregate and utilize context from URS, categorization, planning, and parallel agent outputs in the OQ test generation process.",
          "dependencies": [
            7
          ],
          "details": "Implement logic to collect and merge relevant context from all upstream agents. Ensure the workflow uses this context to generate accurate and complete OQ test scripts.",
          "status": "pending"
        },
        {
          "id": 9,
          "title": "Validate Test Output Quality",
          "description": "Ensure generated OQ test scripts meet pharmaceutical standards and comprehensively cover all requirements.",
          "dependencies": [
            8
          ],
          "details": "Develop validation logic and review processes to check that OQ test scripts are compliant, complete, and aligned with GAMP and regulatory expectations.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 6,
      "title": "Complete SME Agent Implementation",
      "description": "Implement the SME (Subject Matter Expert) agent for domain expertise",
      "details": "Complete the partially implemented SME agent in main/src/agents/parallel/sme_agent.py. The agent should provide pharmaceutical domain expertise, validation patterns, and best practices. Must follow existing parallel agent patterns and integrate with the planner's coordination requests.",
      "testStrategy": "",
      "status": "pending",
      "dependencies": [
        5
      ],
      "priority": "medium",
      "subtasks": []
    },
    {
      "id": 7,
      "title": "Complete Research Agent Implementation",
      "description": "Implement the Research agent for regulatory updates and best practices",
      "details": "Complete the partially implemented Research agent in main/src/agents/parallel/research_agent.py. The agent should provide regulatory updates, current best practices, and industry standards. Must follow existing parallel agent patterns and integrate with the planner's coordination requests.",
      "testStrategy": "",
      "status": "pending",
      "dependencies": [
        5
      ],
      "priority": "medium",
      "subtasks": []
    },
    {
      "id": 8,
      "title": "Fix Parallel Agent Coordination Gap",
      "description": "Connect planner agent requests to actual parallel agent execution",
      "details": "The planner generates parallel agent coordination requests but returns placeholder responses. Need to bridge the gap between planning and execution by implementing proper agent factory and execution pipeline. Ensure SME, Research, and Test Generator agents are actually invoked when requested by the planner.",
      "testStrategy": "",
      "status": "pending",
      "dependencies": [
        6,
        7
      ],
      "priority": "medium",
      "subtasks": []
    },
    {
      "id": 9,
      "title": "End-to-End Workflow Testing with Phoenix",
      "description": "Perform comprehensive integration testing of the complete workflow",
      "details": "Test the entire pharmaceutical test generation workflow end-to-end with all agents integrated. Validate Phoenix observability is capturing all events, traces, and metrics. Test with various URS documents including edge cases. Ensure compliance with GAMP-5, ALCOA+, and 21 CFR Part 11 requirements.",
      "testStrategy": "",
      "status": "pending",
      "dependencies": [
        8
      ],
      "priority": "medium",
      "subtasks": []
    }
  ]
}