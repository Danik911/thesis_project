#!/usr/bin/env python3
"""
Single URS Document Mock API Test

This script tests the cross-validation framework with a single URS document
using MOCKED API responses to validate integration without costs.

Usage:
    python test_single_urs_mock.py

This test costs $0.00 - uses mock responses instead of real API calls.
"""

import asyncio
import json
import logging
import sys
import time
from datetime import datetime, UTC
from pathlib import Path
from typing import Dict, Any
from unittest.mock import Mock, patch, AsyncMock

# Add main directory to Python path
sys.path.insert(0, str(Path(__file__).parent / "main"))

from src.cross_validation.fold_manager import FoldManager
from src.cross_validation.metrics_collector import MetricsCollector
from src.cross_validation.coverage_analyzer import CoverageAnalyzer
from src.cross_validation.quality_metrics import QualityMetrics
from src.cross_validation.statistical_analyzer import StatisticalAnalyzer
from src.cross_validation.results_aggregator import ResultsAggregator

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_mock_test_suite():
    """Create a realistic mock test suite response."""
    return {
        "metadata": {
            "generated_at": datetime.now(UTC).isoformat(),
            "suite_id": "OQ-SUITE-MOCK",
            "gamp_category": 3,
            "total_test_count": 10,
            "compliance_standards": ["GAMP-5", "ALCOA+", "21 CFR Part 11"]
        },
        "test_suite": {
            "suite_id": "OQ-SUITE-MOCK",
            "gamp_category": 3,
            "document_name": "URS-001.md",
            "test_cases": [
                {
                    "test_id": f"OQ-{i:03d}",
                    "test_name": f"Test Case {i}",
                    "test_category": ["installation", "operational", "performance"][i % 3],
                    "gamp_category": 3,
                    "objective": f"Verify requirement {i}",
                    "prerequisites": ["System installed", "User logged in"],
                    "test_steps": [
                        {
                            "step_number": 1,
                            "action": f"Perform action {i}",
                            "expected_result": "Success",
                            "data_to_capture": ["Result", "Timestamp"],
                            "verification_method": "visual_inspection"
                        }
                    ],
                    "acceptance_criteria": [f"Criterion {i} met"],
                    "regulatory_basis": ["21 CFR Part 11"]
                }
                for i in range(1, 11)
            ]
        }
    }


def create_mock_workflow_result():
    """Create a mock workflow result object."""
    result = Mock()
    result.test_suite = create_mock_test_suite()
    result.gamp_category = 3
    result.confidence_score = 0.92
    result.prompt_tokens = 1500
    result.completion_tokens = 2500
    result.total_tokens = 4000
    result.processing_time = 3.5
    return result


async def test_cross_validation_components():
    """Test all cross-validation components with mock data."""
    
    logger.info("=" * 60)
    logger.info("CROSS-VALIDATION COMPONENT TESTING (MOCK)")
    logger.info("=" * 60)
    
    test_results = {
        "timestamp": datetime.now(UTC).isoformat(),
        "tests_passed": 0,
        "tests_failed": 0,
        "components": {}
    }
    
    try:
        # Test 1: FoldManager
        logger.info("\n[TEST 1] FoldManager")
        fold_manager = FoldManager(
            fold_assignments_path=Path("datasets/cross_validation/fold_assignments.json"),
            urs_corpus_path=Path("datasets/urs_corpus"),
            random_seed=42
        )
        
        # Validate fold structure
        assert fold_manager.get_fold_count() == 5, "Should have 5 folds"
        assert len(fold_manager.get_document_inventory()) == 17, "Should have 17 documents"
        
        # Check no data leakage
        all_val_docs = []
        for fold_id, train_docs, val_docs in fold_manager.iterate_folds():
            all_val_docs.extend([doc.document_id for doc in val_docs])
        
        assert len(all_val_docs) == len(set(all_val_docs)), "No duplicates in validation sets"
        
        logger.info("‚úÖ FoldManager: PASS")
        test_results["tests_passed"] += 1
        test_results["components"]["FoldManager"] = "PASS"
        
    except Exception as e:
        logger.error(f"‚ùå FoldManager: FAIL - {e}")
        test_results["tests_failed"] += 1
        test_results["components"]["FoldManager"] = f"FAIL: {e}"
    
    try:
        # Test 2: MetricsCollector
        logger.info("\n[TEST 2] MetricsCollector")
        metrics_collector = MetricsCollector(
            experiment_id="test_mock",
            output_directory=Path("main/output/mock_test")
        )
        
        # Test timing
        timer_key = metrics_collector.start_document_processing(
            document_id="URS-001",
            fold_id="fold_1",
            category_folder="category_3"
        )
        
        time.sleep(0.1)  # Simulate processing
        
        doc_metrics = metrics_collector.end_document_processing(
            timer_key=timer_key,
            success=True,
            prompt_tokens=1500,
            completion_tokens=2500,
            gamp_category=3,
            confidence_score=0.92,
            test_count=10
        )
        
        assert doc_metrics.processing_time > 0, "Processing time should be positive"
        assert doc_metrics.total_tokens == 4000, "Total tokens should be sum"
        
        # Test cost calculation
        cost = metrics_collector.calculate_cost(4000)
        assert cost > 0, "Cost should be positive"
        assert cost < 0.01, "Cost for 4000 tokens should be < $0.01"
        
        logger.info(f"‚úÖ MetricsCollector: PASS (Cost: ${cost:.4f})")
        test_results["tests_passed"] += 1
        test_results["components"]["MetricsCollector"] = "PASS"
        
    except Exception as e:
        logger.error(f"‚ùå MetricsCollector: FAIL - {e}")
        test_results["tests_failed"] += 1
        test_results["components"]["MetricsCollector"] = f"FAIL: {e}"
    
    try:
        # Test 3: CoverageAnalyzer
        logger.info("\n[TEST 3] CoverageAnalyzer")
        coverage_analyzer = CoverageAnalyzer()
        
        # Test with mock URS content
        mock_urs = """
        # Requirements
        1. REQ-001: System shall perform function A
        2. REQ-002: System shall perform function B
        3. REQ-003: System shall validate input C
        """
        
        requirements = coverage_analyzer.extract_requirements(mock_urs)
        assert len(requirements) > 0, "Should extract requirements"
        
        # Test mapping
        mock_tests = create_mock_test_suite()["test_suite"]["test_cases"]
        mapping = coverage_analyzer.map_tests_to_requirements(mock_tests, requirements)
        
        # Calculate coverage
        coverage = coverage_analyzer.calculate_coverage(mapping)
        assert 0 <= coverage <= 100, "Coverage should be percentage"
        
        logger.info(f"‚úÖ CoverageAnalyzer: PASS (Coverage: {coverage:.1f}%)")
        test_results["tests_passed"] += 1
        test_results["components"]["CoverageAnalyzer"] = f"PASS (Coverage: {coverage:.1f}%)"
        
    except Exception as e:
        logger.error(f"‚ùå CoverageAnalyzer: FAIL - {e}")
        test_results["tests_failed"] += 1
        test_results["components"]["CoverageAnalyzer"] = f"FAIL: {e}"
    
    try:
        # Test 4: QualityMetrics
        logger.info("\n[TEST 4] QualityMetrics")
        quality_metrics = QualityMetrics()
        
        # Test confusion matrix
        mock_predictions = [3, 3, 4, 5, 3, 4, 5, 3, 4, 5]
        mock_actual = [3, 3, 4, 5, 3, 4, 5, 4, 4, 5]
        
        cm = quality_metrics.calculate_confusion_matrix(mock_predictions, mock_actual)
        assert cm is not None, "Should create confusion matrix"
        
        # Test metrics calculation
        metrics = quality_metrics.calculate_classification_metrics(cm)
        assert "accuracy" in metrics, "Should calculate accuracy"
        assert 0 <= metrics["accuracy"] <= 1, "Accuracy should be [0,1]"
        
        logger.info(f"‚úÖ QualityMetrics: PASS (Accuracy: {metrics['accuracy']:.2f})")
        test_results["tests_passed"] += 1
        test_results["components"]["QualityMetrics"] = f"PASS (Accuracy: {metrics['accuracy']:.2f})"
        
    except Exception as e:
        logger.error(f"‚ùå QualityMetrics: FAIL - {e}")
        test_results["tests_failed"] += 1
        test_results["components"]["QualityMetrics"] = f"FAIL: {e}"
    
    try:
        # Test 5: StatisticalAnalyzer
        logger.info("\n[TEST 5] StatisticalAnalyzer")
        stat_analyzer = StatisticalAnalyzer()
        
        # Test paired t-test
        fold_results = [
            {"time_reduction": 0.65, "coverage": 0.88},
            {"time_reduction": 0.72, "coverage": 0.92},
            {"time_reduction": 0.68, "coverage": 0.90},
            {"time_reduction": 0.70, "coverage": 0.91},
            {"time_reduction": 0.71, "coverage": 0.89}
        ]
        
        result = stat_analyzer.paired_t_test(fold_results, "time_reduction", 0.70)
        assert "p_value" in result, "Should calculate p-value"
        assert 0 <= result["p_value"] <= 1, "P-value should be [0,1]"
        
        # Test confidence intervals
        values = [r["time_reduction"] for r in fold_results]
        ci = stat_analyzer.calculate_confidence_intervals(values)
        assert "mean" in ci, "Should calculate mean"
        assert "ci_lower" in ci and "ci_upper" in ci, "Should have CI bounds"
        
        logger.info(f"‚úÖ StatisticalAnalyzer: PASS (p={result['p_value']:.4f})")
        test_results["tests_passed"] += 1
        test_results["components"]["StatisticalAnalyzer"] = f"PASS (p={result['p_value']:.4f})"
        
    except Exception as e:
        logger.error(f"‚ùå StatisticalAnalyzer: FAIL - {e}")
        test_results["tests_failed"] += 1
        test_results["components"]["StatisticalAnalyzer"] = f"FAIL: {e}"
    
    try:
        # Test 6: ResultsAggregator
        logger.info("\n[TEST 6] ResultsAggregator")
        aggregator = ResultsAggregator(
            experiment_id="test_mock",
            output_directory=Path("main/output/mock_test")
        )
        
        # Create mock fold results
        for fold_id in range(1, 6):
            fold_result = {
                "fold_id": f"fold_{fold_id}",
                "metrics": {
                    "time_reduction": 0.70 + (fold_id * 0.01),
                    "coverage": 0.90 + (fold_id * 0.005),
                    "fp_rate": 0.03,
                    "fn_rate": 0.04
                },
                "success_rate": 1.0
            }
            aggregator.add_fold_result(fold_result)
        
        # Aggregate results
        summary = aggregator.aggregate_results()
        assert "overall_metrics" in summary, "Should have overall metrics"
        assert summary["overall_metrics"]["time_reduction"]["mean"] > 0.70, "Mean time reduction > 70%"
        
        # Check targets
        target_results = aggregator.check_targets()
        assert "time_reduction" in target_results, "Should check time reduction target"
        
        logger.info(f"‚úÖ ResultsAggregator: PASS")
        test_results["tests_passed"] += 1
        test_results["components"]["ResultsAggregator"] = "PASS"
        
    except Exception as e:
        logger.error(f"‚ùå ResultsAggregator: FAIL - {e}")
        test_results["tests_failed"] += 1
        test_results["components"]["ResultsAggregator"] = f"FAIL: {e}"
    
    # Summary
    logger.info("\n" + "=" * 60)
    logger.info("TEST SUMMARY")
    logger.info("=" * 60)
    logger.info(f"Tests Passed: {test_results['tests_passed']}/6")
    logger.info(f"Tests Failed: {test_results['tests_failed']}/6")
    
    for component, status in test_results["components"].items():
        emoji = "‚úÖ" if "PASS" in status else "‚ùå"
        logger.info(f"{emoji} {component}: {status}")
    
    # Save results
    results_path = Path("main/output/mock_test/component_test_results.json")
    results_path.parent.mkdir(parents=True, exist_ok=True)
    with open(results_path, 'w') as f:
        json.dump(test_results, f, indent=2, default=str)
    
    logger.info(f"\nResults saved to: {results_path}")
    
    if test_results["tests_failed"] == 0:
        logger.info("\nüéâ ALL COMPONENT TESTS PASSED!")
        return 0
    else:
        logger.error(f"\n‚ùå {test_results['tests_failed']} TESTS FAILED")
        return 1


async def test_workflow_integration_mock():
    """Test workflow integration with mocked API responses."""
    
    logger.info("\n" + "=" * 60)
    logger.info("WORKFLOW INTEGRATION TEST (MOCK)")
    logger.info("=" * 60)
    
    # Mock the UnifiedTestGenerationWorkflow
    with patch('src.core.unified_workflow.UnifiedTestGenerationWorkflow') as MockWorkflow:
        # Configure mock
        mock_instance = AsyncMock()
        mock_instance.run = AsyncMock(return_value=create_mock_workflow_result())
        MockWorkflow.return_value = mock_instance
        
        # Import after patching
        from src.cross_validation.cross_validation_workflow import CrossValidationWorkflow
        
        # Create workflow
        workflow = CrossValidationWorkflow(timeout=60, enable_phoenix=False)
        
        # Test with single document
        from src.cross_validation.fold_manager import URSDocument
        test_doc = URSDocument(
            document_id="URS-001",
            category_folder="category_3",
            file_path=Path("datasets/urs_corpus/category_3/URS-001.md"),
            content="Mock URS content"
        )
        
        # Run workflow
        logger.info("Running mocked workflow...")
        start_time = time.perf_counter()
        
        # Simulate processing
        result = await mock_instance.run(
            urs_content=test_doc.content,
            urs_file_path=str(test_doc.file_path)
        )
        
        end_time = time.perf_counter()
        
        # Validate result
        assert result.test_suite is not None, "Should have test suite"
        assert result.gamp_category == 3, "Should have GAMP category"
        assert result.confidence_score > 0.8, "Should have high confidence"
        
        logger.info(f"‚úÖ Workflow completed in {end_time - start_time:.2f}s (mocked)")
        logger.info(f"‚úÖ Generated {len(result.test_suite['test_suite']['test_cases'])} tests")
        logger.info(f"‚úÖ GAMP Category: {result.gamp_category}")
        logger.info(f"‚úÖ Confidence: {result.confidence_score:.2f}")
        logger.info(f"‚úÖ Mock tokens: {result.total_tokens}")
        
        logger.info("\nüéâ WORKFLOW INTEGRATION TEST PASSED (MOCK)!")
        return 0


async def main():
    """Run all mock tests."""
    logger.info("\n" + "üß™" * 30)
    logger.info("CROSS-VALIDATION MOCK TESTING SUITE")
    logger.info("üß™" * 30)
    logger.info("\nThis test validates integration WITHOUT real API calls")
    logger.info("Cost: $0.00\n")
    
    # Run component tests
    component_result = await test_cross_validation_components()
    
    # Run workflow integration test
    workflow_result = await test_workflow_integration_mock()
    
    # Final summary
    if component_result == 0 and workflow_result == 0:
        logger.info("\n" + "=" * 60)
        logger.info("‚úÖ ALL MOCK TESTS PASSED")
        logger.info("=" * 60)
        logger.info("The cross-validation framework is structurally sound.")
        logger.info("Ready for real API testing with test_single_urs_real.py")
        return 0
    else:
        logger.error("\n" + "=" * 60)
        logger.error("‚ùå SOME MOCK TESTS FAILED")
        logger.error("=" * 60)
        logger.error("Please fix issues before running real API tests")
        return 1


if __name__ == "__main__":
    sys.exit(asyncio.run(main()))